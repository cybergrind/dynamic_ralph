[
  {
    "id": "US-001",
    "title": "Add post-run code quality checks to scoring",
    "description": "Add post-run code quality checks to the benchmark scoring system.\n\nAfter the agent finishes a benchmark run, run `uv run pre-commit run -a` and `uv run lint-imports` on the working tree. Add `precommit_clean` (bool) and `lint_imports_clean` (bool) to results JSON.\n\n**Implementation:**\n1. Add `run_quality_checks()` function that runs both commands via subprocess, returns `{\"precommit_clean\": bool, \"lint_imports_clean\": bool}`\n2. Call it in `run_benchmark()` after `git_diff_stat()` (step 4) and before `score_run()` (step 6)\n3. Merge quality results into the results dict\n4. Update `score_run()` to accept `precommit_clean` and `lint_imports_clean` params (default `True` for backward compat)\n5. \"pass\" score now requires both quality checks to pass\n6. Update per-run summary: add `Quality:` line (e.g., `Quality: precommit=OK lint-imports=OK`)\n7. Update `print_scorecard()` to add a `Quality` column",
    "acceptanceCriteria": [
      "`run_quality_checks()` function exists and runs both commands",
      "Results JSON contains `precommit_clean` (bool) and `lint_imports_clean` (bool)",
      "`score_run()` requires both quality checks for 'pass' score",
      "Backward compatible: old results without quality fields default to True",
      "Per-run summary and scorecard display quality status",
      "Tests pass",
      "Lint passes (ruff check)"
    ],
    "priority": 1,
    "passes": true,
    "notes": ""
  },
  {
    "id": "US-002",
    "title": "Add test-writing detection metrics",
    "description": "Add test-writing detection to the benchmark system to measure whether the agent writes tests.\n\n**Implementation:**\n1. Add `expects_tests` field to task definitions in `benchmarks/tasks.json` \u2014 set `false` for existing task-1 through task-6\n2. Add `detect_test_writing(base_commit: str)` function that:\n   - Runs `git diff --name-only <base_commit>` to get changed files\n   - Counts files matching `tests/**` or `test_*.py` patterns \u2192 `test_files_created` (int)\n   - Runs `git diff <base_commit>` and counts lines matching `^\\+\\s*def test_` \u2192 `test_functions_added` (int)\n   - Returns `{\"test_files_created\": int, \"test_functions_added\": int, \"tests_written\": bool}`\n3. Call `detect_test_writing(base_commit)` in `run_benchmark()` after `git_diff_stat()`\n4. Merge test-writing results into the results dict\n5. Add summary line: `Tests written: N functions in M files`",
    "acceptanceCriteria": [
      "`detect_test_writing()` function exists and analyzes git diff",
      "Results JSON contains `test_files_created` (int), `test_functions_added` (int), `tests_written` (bool)",
      "Existing tasks (task-1 through task-6) have `expects_tests: false` in tasks.json",
      "Per-run summary prints 'Tests written: N functions in M files'",
      "Tests pass",
      "Lint passes (ruff check)"
    ],
    "priority": 2,
    "passes": true,
    "notes": ""
  },
  {
    "id": "US-003",
    "title": "Micro-task: fix format_bytes boundary bug",
    "description": "Add task-7 to `benchmarks/tasks.json` \u2014 a micro-task to test agent efficiency on a simple bug fix.\n\nThe bug is real: `octo_server/common/helpers/traffic.py:20` \u2014 `format_bytes()` uses `>` instead of `>=` for all 4 threshold comparisons. `format_bytes(1073741824)` returns `\"1024.00 MiB\"` instead of `\"1.00 GiB\"`.\n\nTask definition:\n```json\n{\n  \"id\": \"task-7\",\n  \"name\": \"fix-format-bytes-bug\",\n  \"difficulty\": \"easy\",\n  \"prompt\": \"Fix the boundary bug in `octo_server/common/helpers/traffic.py` in the `format_bytes()` function. The function uses `>` instead of `>=` for all 4 threshold comparisons (TERABYTE, GIGABYTE, MEGABYTE, KILOBYTE). This means exact boundary values like `format_bytes(1073741824)` return '1024.00 MiB' instead of '1.00 GiB'. Change all 4 comparisons from `>` to `>=`. Run `uv run pre-commit run -a` after fixing.\",\n  \"context\": \"Target file: octo_server/common/helpers/traffic.py. The function is at line 20. It has 4 threshold checks using `>` that should be `>=`. The constants are imported from octo_server/const.py: KILOBYTE=1024, MEGABYTE=1024**2, GIGABYTE=1024**3, TERABYTE=1024**4. This is a 4-line change (> to >= on lines 21-28). Do NOT read unrelated files. The system prompt already contains project conventions.\",\n  \"expected_files\": [\"octo_server/common/helpers/traffic.py\"],\n  \"forbidden_files\": [\"bin/*\", \"migrations/*\", \"tests/*\"],\n  \"max_turns\": 8,\n  \"expects_tests\": false\n}\n```",
    "acceptanceCriteria": [
      "task-7 entry exists in benchmarks/tasks.json with all required fields",
      "Task prompt clearly describes the bug and expected fix",
      "max_turns set to 8",
      "expected_files and forbidden_files are correct",
      "expects_tests is false",
      "Tests pass",
      "Lint passes (ruff check)"
    ],
    "priority": 3,
    "passes": true,
    "notes": ""
  },
  {
    "id": "US-004",
    "title": "Micro-task: write tests for tag query functions",
    "description": "Add task-8 to `benchmarks/tasks.json` \u2014 a micro-task to test the agent's ability to write tests when explicitly instructed.\n\nTarget: `octo_server/core/tags/queries.py` has 9 query functions with zero test coverage.\n\nTask definition:\n```json\n{\n  \"id\": \"task-8\",\n  \"name\": \"write-tag-query-tests\",\n  \"difficulty\": \"medium\",\n  \"prompt\": \"Write tests for the tag query functions in `octo_server/core/tags/queries.py`. The file has 9 query functions with no test coverage. Create a test file at `tests/test_core/test_tags/test_queries.py`. Use the existing `TagFactory` (from `tests/factories.py`, line 509) and `TeamFactory` for test data. Test at least: `get_tag_by_uuid`, `get_tag_by_name`, `get_tags_for_team`, `get_tags_count_for_team`, and `tag_name_exists`. Follow the project's async test patterns (asyncio_mode=auto, use `dbsession` fixture). Run tests with `./bin/run_agent_tests.sh tests/test_core/test_tags/test_queries.py`.\",\n  \"context\": \"Target file to create: tests/test_core/test_tags/test_queries.py. Source: octo_server/core/tags/queries.py has 9 functions: get_tag_by_uuid, get_tag_by_name, get_tags_by_uuids, get_tags_by_names, get_tag_with_profiles, get_tags_for_team, get_tags_count_for_team, tag_name_exists, tag_original_name_exists_with_any_deletion_state. Factories: TagFactory(team=team, name='...', color='grey') at tests/factories.py:509, TeamFactory at line 157. Use `dbsession` and `adopt_dbsession` fixtures. All tests are async by default. Existing test pattern: see tests/test_core/test_tags/test_actions.py for reference. The Tag model has fields: uuid, name, color, deleted_at, team_id. Do NOT read unrelated files. The system prompt already contains project conventions.\",\n  \"expected_files\": [\"tests/test_core/test_tags/test_queries.py\"],\n  \"forbidden_files\": [\"bin/*\", \"migrations/*\", \"octo_server/**/*.py\"],\n  \"max_turns\": 15,\n  \"expects_tests\": true\n}\n```",
    "acceptanceCriteria": [
      "task-8 entry exists in benchmarks/tasks.json with all required fields",
      "Task prompt explicitly instructs writing tests",
      "expects_tests is true",
      "expected_files points to the test file to be created",
      "forbidden_files prevents modifying production code",
      "Tests pass",
      "Lint passes (ruff check)"
    ],
    "priority": 4,
    "passes": true,
    "notes": ""
  },
  {
    "id": "US-005",
    "title": "Micro-task: add search parameter to tags endpoint",
    "description": "Add task-9 to `benchmarks/tasks.json` \u2014 a micro-task that tests whether the agent writes tests unprompted.\n\nTarget: Add optional `search` query parameter to `GET /tags` endpoint at `octo_server/api/client/tags/router.py:40` with `ilike` filter in `octo_server/core/tags/queries.py:87`.\n\nThe prompt deliberately does NOT say \"write tests\" \u2014 this measures unprompted test-writing.\n\nTask definition:\n```json\n{\n  \"id\": \"task-9\",\n  \"name\": \"add-tag-search-param\",\n  \"difficulty\": \"medium\",\n  \"prompt\": \"Add an optional `search` query parameter to the `GET /tags` endpoint. When provided, filter tags whose name contains the search string (case-insensitive). Modify `get_tags_view` in `octo_server/api/client/tags/router.py` to accept `search: str | None = None` and pass it to `get_tags_for_team`. Update `get_tags_for_team` in `octo_server/core/tags/queries.py` to accept an optional `search` parameter and apply an `ilike` filter (`Tag.name.ilike(f'%{search}%')`) when provided. Run `uv run pre-commit run -a` after making changes.\",\n  \"context\": \"Target files: octo_server/api/client/tags/router.py (get_tags_view at line 40) and octo_server/core/tags/queries.py (get_tags_for_team at line 87). The endpoint currently calls `get_tags_for_team(dbsession=dbsession, team_id=current_user.team_id)`. Add `search: str | None = None` as a query parameter to `get_tags_view` and pass it through. In `get_tags_for_team`, add `search: str | None = None` parameter and if provided, append `Tag.name.ilike(f'%{search}%')` to the where clause. Existing tests: tests/test_api/test_client/test_tags/test_router.py. Do NOT read unrelated files. The system prompt already contains project conventions.\",\n  \"expected_files\": [\"octo_server/api/client/tags/router.py\", \"octo_server/core/tags/queries.py\"],\n  \"forbidden_files\": [\"bin/*\", \"migrations/*\"],\n  \"max_turns\": 18,\n  \"expects_tests\": true\n}\n```",
    "acceptanceCriteria": [
      "task-9 entry exists in benchmarks/tasks.json with all required fields",
      "Task prompt does NOT mention writing tests",
      "expects_tests is true for analysis purposes",
      "expected_files includes both router and query files",
      "Tests pass",
      "Lint passes (ruff check)"
    ],
    "priority": 5,
    "passes": true,
    "notes": ""
  },
  {
    "id": "US-006",
    "title": "Run baseline evaluation on micro-tasks",
    "description": "Run a baseline evaluation on the 3 new micro-tasks to capture current agent performance.\n\nRun: `uv run bin/benchmark_task.py --tasks task-7,task-8,task-9 --iteration baseline-micro --prebuild --max-cost 3.00`\n\nVerify:\n- All 3 tasks complete with results JSON\n- Results include quality checks (precommit_clean, lint_imports_clean) and test-writing metrics (test_files_created, test_functions_added, tests_written)\n- Generate reports for each task",
    "acceptanceCriteria": [
      "All 3 tasks (task-7, task-8, task-9) complete with results JSON files",
      "Results JSON includes quality and test-writing metric fields",
      "REPORT.md generated for each task",
      "Tests pass",
      "Lint passes (ruff check)"
    ],
    "priority": 6,
    "passes": true,
    "notes": ""
  },
  {
    "id": "US-007",
    "title": "Analyze findings and document prompt/doc gaps",
    "description": "Review conversation logs, results, and reports from the baseline evaluation (US-006). Create `benchmarks/ANALYSIS.md` with per-task breakdown:\n\n1. **Pass/fail:** Did the task pass? How many turns vs budget?\n2. **Tool call breakdown:** Which tools were used and how many times?\n3. **Style compliance:** Did the agent run pre-commit? Did code pass quality checks?\n4. **Test writing:** Did the agent write tests for task-8 (instructed) and task-9 (unprompted)?\n5. **Efficiency:** Unnecessary file reads, directory exploration, re-reading CLAUDE.md?\n\nIdentify at least 3 concrete prompt/doc gaps with specific log evidence.",
    "acceptanceCriteria": [
      "benchmarks/ANALYSIS.md exists with per-task breakdown",
      "Breakdown includes all 5 dimensions: pass/fail, tools, style, tests, efficiency",
      "At least 3 concrete prompt/doc gaps identified with log evidence",
      "Tests pass",
      "Lint passes (ruff check)"
    ],
    "priority": 7,
    "passes": true,
    "notes": ""
  },
  {
    "id": "US-008",
    "title": "Apply prompt/doc fixes and validate with re-run",
    "description": "Based on US-007 findings, update `multi_agent/prompts.py` (BASE_AGENT_INSTRUCTIONS) and/or `docs/code_guide.md`.\n\nRe-run: `uv run bin/benchmark_task.py --tasks task-7,task-8,task-9 --iteration improved-micro --prebuild --max-cost 3.00`\n\nCompare iterations to measure improvement across 3 dimensions:\n- Efficiency (fewer turns)\n- Style compliance (quality checks pass)\n- Test writing (more tests written)",
    "acceptanceCriteria": [
      "Prompt/doc changes applied based on US-007 findings",
      "Re-run completed with --iteration improved-micro",
      "At least 2 of 3 dimensions show measurable improvement vs baseline",
      "uv run pre-commit run -a passes",
      "Tests pass",
      "Lint passes (ruff check)"
    ],
    "priority": 8,
    "passes": true,
    "notes": ""
  }
]
